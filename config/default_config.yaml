# CLIP 모델 설정
model:
  name: "openai/clip-vit-base-patch32"
  device: "auto"  # cuda, cpu, mps, auto

# 이미지 처리 설정
image:
  supported_formats: [".jpg", ".jpeg", ".png"]
  max_size: 224  # CLIP 모델의 기본 입력 크기

# 클러스터링 설정
clustering:
  threshold: 0.95
  batch_size: 1000

# 출력 설정
output:
  embedding_dir: "sample_data/embeddings"
  result_dir: "results/outputs"
  visualization_dir: "results/visualization" 